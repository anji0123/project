from pyspark.sql import SparkSession
from pyspark.sql.functions import col, window, expr
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
schema = StructType([
    StructField("timestamp", TimestampType(), True),
    StructField("user_id", StringType(), True),
    StructField("action", StringType(), True)
])
spark = SparkSession.builder.appName("UserSessionTracking").getOrCreate()
sample_data_df = spark.readStream.schema(schema).csv("/FileStore/tables/")
from pyspark.sql.functions import approx_count_distinct,collect_list

sessionized_stream = sample_data_df \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(window("timestamp", "10 minutes")) \
    .agg(approx_count_distinct("user_id").alias("session_count"),collect_list("action").alias("actions"))

query = sessionized_stream.writeStream \
    .format("console").outputMode("update").trigger(processingTime="10 seconds").start()


 
